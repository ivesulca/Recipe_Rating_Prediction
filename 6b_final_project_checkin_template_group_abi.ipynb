{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Check-in\" data-toc-modified-id=\"Final-Project-Check-in-1\">Final Project Check-in</a></span></li><li><span><a href=\"#Group-Name\" data-toc-modified-id=\"Group-Name-2\">Group Name</a></span></li><li><span><a href=\"#Student-Names\" data-toc-modified-id=\"Student-Names-3\">Student Names</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4\">Load Data</a></span></li><li><span><a href=\"#Fit-scikit-learn-model\" data-toc-modified-id=\"Fit-scikit-learn-model-5\">Fit scikit-learn model</a></span></li><li><span><a href=\"#Evaluation-Metric\" data-toc-modified-id=\"Evaluation-Metric-6\">Evaluation Metric</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project Check-in\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Name\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Grace Hoppers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Names\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Akansha Shrivastava\n",
    "2. Ivette Sulca\n",
    "3. Bing Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import geonamescache  #sudo pip install geonamescache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd=pd.read_csv('../../data/epi_r.csv')\n",
    "data_json=pd.read_json('../../data/full_format_recipes.json',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting null predictive variables from the dataset\n",
    "data_pd=data_pd.loc[(data_pd.rating>0) & (~data_pd.rating.isna())]\n",
    "\n",
    "#Deleting drinks\n",
    "data_pd=data_pd.loc[(data_pd.drink==0) & (data_pd.drinks==0) & (data_pd.cocktail==0)]\n",
    "\n",
    "#Deleting repeted titles\n",
    "data_pd.drop_duplicates(subset=['title'],inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out X and Y\n",
    "X = pd.concat([data_pd.iloc[:,0], data_pd.iloc[:,2:]], axis=1)\n",
    "y = data_pd.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature: n_words: Proxy feature for complexity (Sum across word features, from alabama to zuccini)\n",
    "X[\"n_words\"] = X.iloc[:,11:673].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature: holidays: Dummy for world holiday \n",
    "# (a celebratory day attached to date in calendar-- birthday, graduation, anniversary, etc. not included)\n",
    "holidays = [\"bastille day\", \"christmas\", \"christmas eve\", \"cinco de mayo\", \"columbus\", \n",
    " \"diwali\", \"easter\", \"father's day\", \"fourth of july\", \"friendsgiving\", \"halloween\",\n",
    " \"hanukkah\", \"kwanzaa\", \"labor day\", \"lunar new year\", \"mother's day\", \"new year's day\",\n",
    " \"new year's eve\", \"oktoberfest\", \"passover\", \"persian new year\", \"purim\", \"ramadan\", \n",
    " \"rosh hashanah/yom kippur\", \"st. patrick's day\", \"sukkot\", \"thanksgiving\", \n",
    " \"valentine's day\"]\n",
    "\n",
    "X[\"holiday\"] = np.where(X[holidays].sum(axis=1)>0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAT: In grams but it can mislead depending of the number of portions (Paella for example)\n",
    "\n",
    "#Delete bigger portions(Paella) and outliers\n",
    "y=y.loc[((X.fat>=0) & (X.fat<=200)) | (X.fat.isna())]\n",
    "X=X.loc[((X.fat>=0) & (X.fat<=200)) | (X.fat.isna())]\n",
    "\n",
    "#Median imputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "fat_clean = imp.fit_transform(X.fat.values.reshape(-1,1))\n",
    "fat_clean = pd.DataFrame(data=fat_clean ,columns=['fat_clean'])\n",
    "X['fat']=fat_clean.fat_clean.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROTEIN CLEANING\n",
    "\n",
    "#Again, considering values lower than 200: deleting 62 rows...\n",
    "y=y.loc[((X.protein>=0) & (X.protein<=200)) | (X.protein.isna())]\n",
    "X=X.loc[((X.protein>=0) & (X.protein<=200)) | (X.protein.isna())]\n",
    "\n",
    "#Median imputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "protein_clean = imp.fit_transform(X.protein.values.reshape(-1,1))\n",
    "protein_clean = pd.DataFrame(data=protein_clean ,columns=['protein_clean'])\n",
    "X['protein']=protein_clean.protein_clean.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SODIUM\n",
    "\n",
    "#Unit: miligrams\n",
    "#Very different values, so we will impute only:\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "sodium_clean = imp.fit_transform(X.sodium.values.reshape(-1,1))\n",
    "sodium_clean = pd.DataFrame(data=sodium_clean ,columns=['sodium_clean'])\n",
    "X['sodium']=sodium_clean.sodium_clean.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering columns related to \"healthy\"\n",
    "\n",
    "# selecting all the relevant columns\n",
    "\n",
    "selected = [\"fat free\", \"healthy\", \"low cal\", \"quick and healthy\", \n",
    "\"low carb\",\n",
    "\"low cholesterol\",\n",
    "\"low fat\",\n",
    "\"low sodium\",\n",
    "\"low sugar\",\n",
    "\"low/no sugar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering rows which have \"selected\" columns as 1 -> healthy\n",
    "data_pd = X\n",
    "\n",
    "data_pd[\"allhealthy\"] = 0\n",
    "for col in selected:\n",
    "    data_pd.loc[data_pd[col] == 1, \"allhealthy\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing missing values in calories by mean\n",
    "\n",
    "median = data_pd[\"calories\"].median()\n",
    "data_pd.loc[data_pd[\"calories\"].isna(), \"calories\"] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding complexity preparation from JSON file\n",
    "\n",
    "data_json.drop_duplicates(subset=['title'],inplace=True)  \n",
    "data_json['directions_n_characters']=data_json['directions'].astype(str).str.len()\n",
    "data_json['ingredients_quantity']=data_json['ingredients'].str.len()\n",
    "data_json['directions_n_steps']=data_json['directions'].astype(str).str.replace('[','').str.replace(']','').str.split(\"',\").apply(lambda x: len(x))\n",
    "data_json=data_json[['title','directions_n_characters','ingredients_quantity','directions_n_steps']]\n",
    "\n",
    "\n",
    "data_pd=pd.merge(data_pd, data_json, on='title', how=\"left\")\n",
    "\n",
    "#Change by imputer\n",
    "data_pd.loc[data_pd.directions_n_steps.isna(),'directions_n_steps']=0\n",
    "data_pd.loc[data_pd.ingredients_quantity.isna(),'ingredients_quantity']=0\n",
    "data_pd.loc[data_pd.directions_n_characters.isna(),'directions_n_characters']=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15437, 685)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying locations:\n",
    "column_names=np.array(data_pd.columns)\n",
    "column_names=[c.strip().upper() for c in column_names]\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "countries = gc.get_countries_by_names()\n",
    "cities = gc.get_cities()\n",
    "states = gc.get_us_states()\n",
    "\n",
    "dict_countries=dict()\n",
    "for k,v in countries.items():\n",
    "    dict_countries[k.upper()]=[v['geonameid'],v['iso'],v['iso3']]\n",
    "\n",
    "dict_countries2=dict()\n",
    "for k,v in countries.items():\n",
    "    dict_countries2[v['iso'].upper()]=k.upper()\n",
    "\n",
    "dict_cities=dict()\n",
    "for k,v in cities.items():\n",
    "    dict_cities[v['name'].upper()]=[v['geonameid'],v['countrycode']]\n",
    "\n",
    "dict_states=dict()    \n",
    "for k,v in states.items():\n",
    "    dict_states[v['name'].upper()] = [v['geonameid'],v['code'], 'US' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping locations:\n",
    "#DECISION: DROP LOCATION COLUMNS(99 columns, most of them are null and is note helpful)\n",
    "#CHECK AT THE END HOW MANY OF THEM ARE \n",
    "\n",
    "data_pd['country_id']=np.nan\n",
    "data_pd['country_name']=np.nan\n",
    "data_pd['state_id']=np.nan\n",
    "data_pd['state_name']=np.nan\n",
    "data_pd['city_id']=np.nan\n",
    "data_pd['city_name']=np.nan\n",
    "\n",
    "\n",
    "#1. Identifying countries, states and cities\n",
    "\n",
    "list_col_drop=[]\n",
    "for col in data_pd.columns:    \n",
    "    if col.upper() in dict_countries:\n",
    "        if col.upper() not in ['TURKEY']:\n",
    "#            data_pd.loc[data_pd[col]==1,'country_id'] = dict_countries[col.upper()][1]\n",
    "#            data_pd.loc[data_pd[col]==1,'country_name'] = col.upper()   \n",
    "            list_col_drop.append(col)\n",
    " \n",
    "\n",
    "    if col.upper() in dict_states:        \n",
    "#        data_pd.loc[data_pd[col]==1,'state_id'] = dict_states[col.upper()][1]\n",
    "#        data_pd.loc[data_pd[col]==1,'state_name'] = col.upper()   \n",
    "#        data_pd.loc[data_pd[col]==1,'country_id'] = 'US'\n",
    "#        data_pd.loc[data_pd[col]==1,'country_name'] = 'UNITED STATES' \n",
    "        list_col_drop.append(col)\n",
    " \n",
    "    if col.upper() in dict_cities:     \n",
    "        if col.upper() not in ['SPRING','ORANGE','WALNUT','LEEK','WEDDING','PLUM','TEQUILA','DATE','PAPAYA','MARSALA','SAKE','RYE','GOUDA','HOLIDAY']:\n",
    " #           data_pd.loc[data_pd[col]==1,'city_id'] = dict_cities[col.upper()][0]\n",
    " #           data_pd.loc[data_pd[col]==1,'city_name'] = col.upper()   \n",
    " #           data_pd.loc[data_pd[col]==1,'country_id'] = dict_cities[col.upper()][1]\n",
    " #           data_pd.loc[data_pd[col]==1,'country_name'] = dict_countries2[dict_cities[col.upper()][1]] \n",
    "            list_col_drop.append(col)\n",
    "\n",
    "#2. Dropping individual location columns\n",
    "data_pd=data_pd.drop(list_col_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit scikit-learn model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the below features in the model:\n",
    "X = data_pd[[\"calories\", \"fat\", \"protein\", \"sodium\", \"22-minute meals\",\n",
    "       \"3-ingredient recipes\", \"n_words\", \"holiday\", \"allhealthy\",\n",
    "      \"directions_n_characters\"\n",
    "       ,\"ingredients_quantity\",\n",
    "       \"directions_n_steps\"\n",
    "      ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    6966\n",
       "4    4479\n",
       "6    2044\n",
       "3    1281\n",
       "2     445\n",
       "0     123\n",
       "1      99\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y (rating) only appears as 6 different floats between 1 and 4\n",
    "# Convert floats to categorical variables, 0 to 6, for classification\n",
    "\n",
    "y_discrete = pd.cut(y, bins=7, labels=np.arange(7), right=False)\n",
    "y_discrete.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_discrete, test_size=0.20, stratify=y_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivettesulca/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  82    0    0    0    4   11    1]\n",
      " [   0   59    0    0    5   14    1]\n",
      " [   0    0  307    0   10   38    1]\n",
      " [   0    0    0  931   34   59    1]\n",
      " [   0    0    0    0 3519   64    0]\n",
      " [   0    0    0    0   14 5559    0]\n",
      " [   0    0    0    1   40   77 1517]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91        98\n",
      "           1       1.00      0.75      0.86        79\n",
      "           2       1.00      0.86      0.93       356\n",
      "           3       1.00      0.91      0.95      1025\n",
      "           4       0.97      0.98      0.98      3583\n",
      "           5       0.95      1.00      0.98      5573\n",
      "           6       1.00      0.93      0.96      1635\n",
      "\n",
      "    accuracy                           0.97     12349\n",
      "   macro avg       0.99      0.89      0.94     12349\n",
      "weighted avg       0.97      0.97      0.97     12349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(cm)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0   25    0]\n",
      " [   0    0    0    0    1   19    0]\n",
      " [   0    0    0    1    2   86    0]\n",
      " [   0    0    0    0    8  247    1]\n",
      " [   0    0    0    0   34  860    2]\n",
      " [   0    0    0    2   48 1340    3]\n",
      " [   0    0    0    0   18  390    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        25\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.00      0.00      0.00        89\n",
      "           3       0.00      0.00      0.00       256\n",
      "           4       0.31      0.04      0.07       896\n",
      "           5       0.45      0.96      0.61      1393\n",
      "           6       0.14      0.00      0.00       409\n",
      "\n",
      "    accuracy                           0.45      3088\n",
      "   macro avg       0.13      0.14      0.10      3088\n",
      "weighted avg       0.31      0.45      0.30      3088\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivettesulca/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unweighted F1 score overall: \", round(f1_score(y_test, y_pred, average='macro'), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
